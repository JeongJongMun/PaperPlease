{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(model=\"gpt-4-1106-preview\")\n",
    "\n",
    "# obj : ChatPromptTemplate\n",
    "obj = hub.pull(\"hardkothari/prompt-maker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = llm(obj.format_messages(task=\"Return a PDF link to a given AI research paper topic and a description and summary of the paper topic.\", lazy_prompt=\"You are a chatbot that provides a PDF link to the AI research paper topic the user asked about, and provides a summary and description of the PDF link.\"))\n",
    "\n",
    "\n",
    "print(prompt.content)\n",
    "\"\"\"\n",
    "    {context}\n",
    "    {chat_history}\n",
    "    {input}\n",
    "    {agent_scratchpad}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# obj : PromptTemplate\n",
    "llm=ChatOpenAI(model=\"gpt-4-1106-preview\")\n",
    "obj = hub.pull(\"gregkamradt/test-question-making\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = obj.format(context=\"AI Research Paper Summary and Search Chatbot\", question=\"What is Attention is all you need?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stem Example:\n",
      "In the context of machine learning and neural networks, the paper titled \"Attention Is All You Need\" introduced which significant advancement?\n",
      "\n",
      "Answer Choices (Distractors A-C and Correct Answer D) Example:\n",
      "A. An efficient algorithm for gradient descent optimization.\n",
      "B. A novel approach to convolutional neural networks.\n",
      "C. A new activation function that outperforms ReLU.\n",
      "D. The Transformer architecture, which relies solely on attention mechanisms without recurrent or convolutional layers. (correct)\n",
      "\n",
      "Rationale Example:\n",
      "Correct. \"Attention Is All You Need\" introduced the Transformer architecture, which uses self-attention mechanisms and has been influential in the field of natural language processing. This architecture allows for parallel processing of sequences and has led to models like BERT and GPT. See Module 3: Neural Network Architectures.\n",
      "\n",
      "Incorrect. While gradient descent optimization is crucial for training neural networks, \"Attention Is All You Need\" is specifically known for introducing the Transformer architecture, not for an algorithm related to gradient descent. For more information on optimization algorithms, review Module 2: Training Neural Networks.\n",
      "\n",
      "Incorrect. Convolutional neural networks (CNNs) represent an earlier class of neural network architectures, and while they are important, the \"Attention Is All You Need\" paper is not about a novel approach to CNNs. Instead, it introduces an architecture that forgoes convolutional and recurrent layers entirely. To understand the difference between these architectures, see Module 3: Neural Network Architectures.\n",
      "\n",
      "Incorrect. The paper does not introduce a new activation function; it proposes a new neural network architecture that focuses on attention mechanisms. Activation functions like ReLU are fundamental components of many neural network architectures, but they are not the focus of this paper. For a review of activation functions and their roles in neural networks, see Module 2: Training Neural Networks.\n"
     ]
    }
   ],
   "source": [
    "print(output.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
